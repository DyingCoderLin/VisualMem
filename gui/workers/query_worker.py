#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æŸ¥è¯¢å·¥ä½œçº¿ç¨‹ - æ‰§è¡Œ query.py çš„æŸ¥è¯¢é€»è¾‘
"""

from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Tuple
import base64
import io
import requests
import asyncio

from PySide6.QtCore import QObject, Signal
from PIL import Image as PILImage

from config import config
from utils.logger import setup_logger
from core.capture.screenshot_capturer import ScreenshotCapturer
from core.storage.sqlite_storage import SQLiteStorage
from core.understand.api_vlm import ApiVLM
from core.retrieval.query_llm_utils import (
    rewrite_and_time,
)
from core.retrieval.reranker import Reranker

logger = setup_logger("query_worker")


class QueryWorker(QObject):
    """æŸ¥è¯¢å·¥ä½œçº¿ç¨‹"""
    
    result_signal = Signal(str)  # ç»“æœä¿¡å·
    progress_signal = Signal(str)  # è¿›åº¦ä¿¡å·
    error_signal = Signal(str)  # é”™è¯¯ä¿¡å·
    
    def __init__(self, storage_mode: str):
        super().__init__()
        # GUI æ¨¡å¼ï¼ˆlocal / remoteï¼‰
        self.gui_mode = config.GUI_MODE
        self.backend_url = config.GUI_REMOTE_BACKEND_URL.rstrip("/") if config.GUI_REMOTE_BACKEND_URL else ""

        # åœ¨ remote æ¨¡å¼ä¸‹ï¼Œstorage_mode å¯¹æœ¬åœ°æ£€ç´¢æ— æ„ä¹‰ï¼Œä»…ç”¨äºå…¼å®¹æ¥å£
        self.storage_mode = storage_mode
        
        # å»¶è¿Ÿåˆå§‹åŒ–ï¼šåªåœ¨éœ€è¦æ—¶æ‰åŠ è½½é‡å‹ç»„ä»¶
        self.encoder = None
        self.retriever = None
        self.storage = None
        self.frame_cache = None
        self._vector_mode_initialized = False
        self._simple_mode_initialized = False
        
        # VLM ä¹Ÿå»¶è¿Ÿåˆå§‹åŒ–
        self._vlm = None
    
    @property
    def vlm(self):
        """å»¶è¿ŸåŠ è½½ VLM"""
        if self._vlm is None:
            self._vlm = ApiVLM()
        return self._vlm
    
    def _ensure_simple_mode_initialized(self):
        """ç¡®ä¿ Simple æ¨¡å¼ç»„ä»¶å·²åˆå§‹åŒ–ï¼ˆå»¶è¿ŸåŠ è½½ï¼‰"""
        if self.gui_mode == "remote":
            # è¿œç¨‹æ¨¡å¼ä¸‹ä¸åœ¨æœ¬åœ°åˆå§‹åŒ–å­˜å‚¨
            return
        if self._simple_mode_initialized:
            return
        
        self.progress_signal.emit("ğŸ“‚ æ­£åœ¨åˆå§‹åŒ– Simple æ¨¡å¼å­˜å‚¨...")
        from core.storage.simple_storage import SimpleStorage
        from query import FrameCache
        
        self.storage = SimpleStorage(storage_path=config.IMAGE_STORAGE_PATH)
        self.encoder = None
        self.retriever = None
        self.frame_cache = FrameCache(
            max_size=config.MAX_IMAGES_TO_LOAD,
            diff_threshold=config.SIMPLE_FILTER_DIFF_THRESHOLD
        )
        self._simple_mode_initialized = True
    
    def _ensure_vector_mode_initialized(self):
        """ç¡®ä¿ Vector æ¨¡å¼ç»„ä»¶å·²åˆå§‹åŒ–ï¼ˆå»¶è¿ŸåŠ è½½ CLIP æ¨¡å‹ï¼‰"""
        if self.gui_mode == "remote":
            # è¿œç¨‹æ¨¡å¼ä¸‹ä¸åœ¨æœ¬åœ°åŠ è½½ CLIP / LanceDB
            return
        if self._vector_mode_initialized:
            return
        
        self.progress_signal.emit("ğŸ”„ æ­£åœ¨åŠ è½½ CLIP æ¨¡å‹... (é¦–æ¬¡åŠ è½½è¾ƒæ…¢)")
        from core.encoder.clip_encoder import CLIPEncoder
        from core.storage.lancedb_storage import LanceDBStorage
        from core.retrieval.image_retriever import ImageRetriever
        
        self.encoder = CLIPEncoder(model_name=config.CLIP_MODEL)
        self.progress_signal.emit("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ– LanceDB å­˜å‚¨...")
        self.storage = LanceDBStorage(
            db_path=config.LANCEDB_PATH,
            embedding_dim=self.encoder.embedding_dim
        )
        self.retriever = ImageRetriever(
            encoder=self.encoder,
            storage=self.storage,
            top_k=10
        )
        self._vector_mode_initialized = True
        self.progress_signal.emit("âœ… CLIP æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _ensure_storage_only(self):
        """åªåˆå§‹åŒ–å­˜å‚¨ï¼ˆä¸åŠ è½½ CLIP æ¨¡å‹ï¼Œç”¨äºä¸éœ€è¦å‘é‡æ£€ç´¢çš„æŸ¥è¯¢ï¼‰"""
        if self.gui_mode == "remote":
            # è¿œç¨‹æ¨¡å¼ä¸‹ä¸åœ¨æœ¬åœ°åˆå§‹åŒ–å­˜å‚¨
            return
        if self.storage is not None:
            return
        
        self.progress_signal.emit("ğŸ“¦ æ­£åœ¨åˆå§‹åŒ–å­˜å‚¨...")
        if self.storage_mode == "simple":
            from core.storage.simple_storage import SimpleStorage
            self.storage = SimpleStorage(storage_path=config.IMAGE_STORAGE_PATH)
        else:
            from core.storage.lancedb_storage import LanceDBStorage
            self.storage = LanceDBStorage(
                db_path=config.LANCEDB_PATH,
                embedding_dim=512  # é»˜è®¤ç»´åº¦ï¼Œåªç”¨äºè¯»å–
            )
    
    def query_rag(self, query_text: str, top_k: int = None):
        """RAG å¿«é€Ÿæ£€ç´¢ï¼ˆå…¨åº“ï¼Œæ”¯æŒ Hybrid Searchï¼‰"""
        try:
            self.progress_signal.emit("ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³å›¾ç‰‡...")
            logger.info(f"RAGæŸ¥è¯¢: '{query_text}'")
            
            # å»¶è¿Ÿåˆå§‹åŒ–ï¼šåªæœ‰åœ¨çœŸæ­£éœ€è¦æ—¶æ‰åŠ è½½ CLIP æ¨¡å‹
            if self.storage_mode == "simple":
                self._ensure_simple_mode_initialized()
                frames = self._rag_simple_mode(query_text, top_k)
            else:
                self._ensure_vector_mode_initialized()
                
                # è·å–æŸ¥è¯¢é‡å†™ç»“æœ
                dense_queries = [query_text]
                sparse_queries = [query_text]
                if config.ENABLE_LLM_REWRITE:
                    dense_queries, sparse_queries, _ = rewrite_and_time(
                        query_text,
                        enable_rewrite=config.ENABLE_LLM_REWRITE,
                        enable_time=False,
                        expand_n=config.QUERY_REWRITE_NUM,
                    )
                
                # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šDense æœç´¢
                def _dense_search_task():
                    """Dense æœç´¢ä»»åŠ¡ï¼ˆåŒæ­¥å‡½æ•°ï¼‰"""
                    dense_frames = []
                    for q in dense_queries:
                        frame_list = self.retriever.retrieve_by_text(q, top_k=top_k or config.MAX_IMAGES_TO_LOAD)
                        dense_frames.extend(frame_list)
                    # åŠ è½½å›¾ç‰‡
                    for frame in dense_frames:
                        if 'image' not in frame or frame['image'] is None:
                            self._load_frame_image(frame)
                    return dense_frames
                
                # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šSparse æœç´¢
                def _sparse_search_task():
                    """Sparse æœç´¢ä»»åŠ¡ï¼ˆåŒæ­¥å‡½æ•°ï¼‰"""
                    if not config.ENABLE_HYBRID:
                        return []
                    
                    try:
                        from core.retrieval.text_retriever import create_text_retriever
                        # ä¸åˆ›å»º encoderï¼Œå› ä¸º sparse æœç´¢ï¼ˆFTSï¼‰ä¸éœ€è¦
                        text_retriever = create_text_retriever(create_encoder=False)
                        
                        sparse_frames = []
                        for q in sparse_queries:
                            sparse_results = text_retriever.retrieve_sparse(
                                query=q,
                                top_k=top_k or config.MAX_IMAGES_TO_LOAD,
                                text_field="text",
                                filter=None  # å…¨åº“æœç´¢ï¼Œä¸é™åˆ¶æ—¶é—´
                            )
                            
                            for result in sparse_results:
                                score = result.get("_relevance_score") or result.get("_score") or result.get("score", 0.0)
                                if score == 0.0:
                                    continue
                                
                                fid = result.get("frame_id")
                                if not fid:
                                    continue
                                
                                frame = {
                                    "frame_id": fid,
                                    "timestamp": datetime.fromisoformat(result.get("timestamp")) if isinstance(result.get("timestamp"), str) else result.get("timestamp"),
                                    "image_path": result.get("image_path"),
                                    "ocr_text": result.get("text") or result.get("ocr_text", ""),
                                    "distance": 1.0 - score,
                                    "metadata": result.get("metadata", {})
                                }
                                
                                if frame.get("image_path"):
                                    self._load_frame_image(frame)
                                
                                sparse_frames.append(frame)
                        
                        return sparse_frames
                    except Exception as e:
                        logger.error(f"Sparse æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                        self.progress_signal.emit(f"âš ï¸ Sparse æ£€ç´¢å¤±è´¥ï¼Œä»…ä½¿ç”¨ Dense ç»“æœ")
                        return []
                
                # å¹¶è¡Œæ‰§è¡Œ Dense å’Œ Sparse æœç´¢ï¼ˆä½¿ç”¨ asyncioï¼‰
                if config.ENABLE_HYBRID:
                    self.progress_signal.emit("ğŸ” å¯ç”¨ Hybrid Searchï¼Œå¹¶è¡Œæ‰§è¡Œ Dense å’Œ Sparse æ£€ç´¢...")
                    async def _run_parallel_searches():
                        dense_task = asyncio.to_thread(_dense_search_task)
                        sparse_task = asyncio.to_thread(_sparse_search_task)
                        return await asyncio.gather(dense_task, sparse_task)
                    
                    dense_results, sparse_results = asyncio.run(_run_parallel_searches())
                else:
                    dense_results = _dense_search_task()
                    sparse_results = []
                
                # åˆå¹¶ç»“æœå¹¶å»é‡
                frames = []
                seen = set()
                
                for r in dense_results:
                    fid = r.get("frame_id")
                    if fid in seen:
                        continue
                    seen.add(fid)
                    frames.append(r)
                
                for r in sparse_results:
                    fid = r.get("frame_id")
                    if fid in seen:
                        continue
                    seen.add(fid)
                    frames.append(r)
            
            if not frames:
                self.result_signal.emit("æœªæ‰¾åˆ°ç›¸å…³çš„å±å¹•è®°å½•ã€‚")
                return
            
            logger.info(f"æ‰¾åˆ° {len(frames)} ä¸ªå¸§")
            
            # ç¡®ä¿æ‰€æœ‰ frame éƒ½æœ‰å›¾ç‰‡
            frames_with_images = [f for f in frames if f.get("image") is not None]
            if not frames_with_images:
                self.result_signal.emit("æ£€ç´¢åˆ°çš„å›¾ç‰‡æ— æ³•åŠ è½½ã€‚")
                return
            
            # Rerank ç¯èŠ‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.ENABLE_RERANK:
                self.progress_signal.emit(f"ğŸ”„ æ­£åœ¨è¿›è¡Œ Rerankï¼ˆè¿”å› top-{config.RERANK_TOP_K}ï¼‰...")
                reranker = Reranker()
                frames_with_images = reranker.rerank(
                    query=query_text,
                    frames=frames_with_images,
                    top_k=config.RERANK_TOP_K
                )
                
                if not frames_with_images:
                    self.result_signal.emit("Rerank åæ²¡æœ‰å›¾ç‰‡ï¼Œæ— æ³•è¿›è¡Œ VLM åˆ†æã€‚")
                    return
                
                self.progress_signal.emit(f"âœ… Rerank å®Œæˆ: è¿”å› top-{len(frames_with_images)} å¼ å›¾ç‰‡")
            
            # VLM åˆ†æï¼ˆåªä½¿ç”¨é€šè¿‡ rerank çš„å›¾ç‰‡ï¼‰
            response = self._analyze_with_vlm(query_text, frames_with_images)
            result = self._format_rag_result(response, frames_with_images)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"RAGæŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
    
    def query_rag_with_time(self, query_text: str, start_time: datetime, end_time: datetime):
        """RAG æŸ¥è¯¢ï¼ˆå¸¦æ—¶é—´èŒƒå›´è¿‡æ»¤ï¼‰

        - GUI_MODE=local  : ä½¿ç”¨æœ¬åœ° LanceDB + SQLite + Reranker + VLM
        - GUI_MODE=remote : é€šè¿‡ HTTP è°ƒç”¨ backend_serverï¼Œç”±åç«¯å®Œæˆæ£€ç´¢+RAG+rerank+VLM
        """
        try:
            # Ensure start_time and end_time are UTC for database queries
            if start_time and start_time.tzinfo is None:
                start_time = start_time.astimezone(timezone.utc)
            if end_time and end_time.tzinfo is None:
                end_time = end_time.astimezone(timezone.utc)

            self.progress_signal.emit(f"ğŸ” RAGè¯­ä¹‰æ£€ç´¢ï¼ˆæ—¶é—´èŒƒå›´: {start_time.strftime('%m/%d %H:%M')} - {end_time.strftime('%m/%d %H:%M')}ï¼‰...")
            logger.info(f"RAGæ—¶é—´èŒƒå›´æŸ¥è¯¢: '{query_text}' ({start_time} - {end_time})")

            # ---------- Remote GUI æ¨¡å¼ ----------
            if self.gui_mode == "remote":
                if not self.backend_url:
                    err = "GUI_MODE=remote ä½† GUI_REMOTE_BACKEND_URL æœªé…ç½®"
                    logger.error(err)
                    self.error_signal.emit(err)
                    return

                self.progress_signal.emit("ğŸŒ æ­£åœ¨é€šè¿‡è¿œç¨‹åç«¯æ‰§è¡Œæ£€ç´¢...")
                try:
                    payload = {
                        "query": query_text,
                        "start_time": start_time.isoformat(),
                        "end_time": end_time.isoformat(),
                        "ocr_mode": False,
                        "enable_hybrid": config.ENABLE_HYBRID,
                        "enable_rerank": config.ENABLE_RERANK,
                    }
                    url = self.backend_url + "/api/query_rag_with_time"
                    resp = requests.post(url, json=payload, timeout=300)
                    resp.raise_for_status()
                    data = resp.json()
                except Exception as e:
                    logger.error(f"è¿œç¨‹ RAG æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
                    self.error_signal.emit(f"è¿œç¨‹ RAG æŸ¥è¯¢å¤±è´¥: {e}")
                    return

                answer = data.get("answer", "")
                frames_data = data.get("frames", [])

                # å°†è¿”å›çš„ image_base64 è½¬æˆä¸´æ—¶å›¾ç‰‡æ–‡ä»¶ä¾›ç¼©ç•¥å›¾ä½¿ç”¨ï¼ˆå¯é€‰ï¼‰
                image_paths = []
                for idx, f in enumerate(frames_data[:5]):
                    img_b64 = f.get("image_base64")
                    if not img_b64:
                        continue
                    try:
                        img_bytes = base64.b64decode(img_b64)
                        img = PILImage.open(io.BytesIO(img_bytes))
                        tmp_path = Path(config.IMAGE_STORAGE_PATH) / "remote_gui_thumbs"
                        tmp_path.mkdir(parents=True, exist_ok=True)
                        out_path = tmp_path / f"thumb_{idx}.jpg"
                        img.save(out_path, format="JPEG", quality=80)
                        image_paths.append(out_path)
                    except Exception:
                        continue

                # ä½¿ç”¨ç°æœ‰ ResultPanel API å±•ç¤º VLM ç»“æœ + ç¼©ç•¥å›¾
                self.result_signal.emit(answer)
                # ç¼©ç•¥å›¾é€šè¿‡ä¸»çª—å£çš„ result_panel.add_thumbnails è°ƒç”¨æ—¶å†å¤„ç†ï¼Œè¿™é‡Œä»…è¿”å›æ–‡æœ¬
                return

            # ---------- æœ¬åœ° GUI æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰ ----------
            
            # ç¡®ä¿å‘é‡æ¨¡å¼å·²åˆå§‹åŒ–
            if self.storage_mode == "simple":
                self.result_signal.emit("é”™è¯¯ï¼šSimple æ¨¡å¼ä¸æ”¯æŒå¸¦æ—¶é—´èŒƒå›´çš„ RAG æŸ¥è¯¢ï¼Œè¯·åˆ‡æ¢åˆ° Vector æ¨¡å¼ã€‚")
                return
            
            self._ensure_vector_mode_initialized()
            
            # è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼ˆä½¿ç”¨ LanceDB Pre-filteringï¼Œä¸€æ­¥å®Œæˆå‘é‡æœç´¢å’Œæ—¶é—´è¿‡æ»¤ï¼‰
            self.progress_signal.emit("ğŸ” æ­£åœ¨è¿›è¡Œè¯­ä¹‰æ£€ç´¢ï¼ˆLanceDB Pre-filteringï¼‰...")
            top_k = config.MAX_IMAGES_TO_LOAD
            
            # æ”¯æŒæŸ¥è¯¢é‡å†™
            dense_queries = [query_text]
            sparse_queries = [query_text]
            if config.ENABLE_LLM_REWRITE:
                dense_queries, sparse_queries, _ = rewrite_and_time(
                    query_text,
                    enable_rewrite=config.ENABLE_LLM_REWRITE,
                    enable_time=False,  # æ—¶é—´èŒƒå›´å·²ç»é€šè¿‡ start_time/end_time æŒ‡å®š
                    expand_n=config.QUERY_REWRITE_NUM,
                )
            
            # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šDense æœç´¢
            def _dense_search_task():
                """Dense æœç´¢ä»»åŠ¡ï¼ˆåŒæ­¥å‡½æ•°ï¼‰"""
                dense_frames = []
                for q in dense_queries:
                    query_embedding = self.encoder.encode_text(q)
                    res = self.storage.search(
                        query_embedding, 
                        top_k=top_k,
                        start_time=start_time,
                        end_time=end_time
                    )
                    dense_frames.extend(res)
                return dense_frames
            
            # å®šä¹‰è¾…åŠ©å‡½æ•°ï¼šSparse æœç´¢
            def _sparse_search_task():
                """Sparse æœç´¢ä»»åŠ¡ï¼ˆåŒæ­¥å‡½æ•°ï¼‰"""
                if not config.ENABLE_HYBRID:
                    return []
                
                try:
                    from core.retrieval.text_retriever import create_text_retriever
                    # ä¸åˆ›å»º encoderï¼Œå› ä¸º sparse æœç´¢ï¼ˆFTSï¼‰ä¸éœ€è¦
                    text_retriever = create_text_retriever(create_encoder=False)
                    
                    # æ„å»ºæ—¶é—´è¿‡æ»¤æ¡ä»¶ï¼ˆSQL é£æ ¼ï¼‰
                    time_filter = None
                    if start_time is not None or end_time is not None:
                        conditions = []
                        if start_time is not None:
                            start_iso = start_time.isoformat()
                            conditions.append(f"timestamp >= '{start_iso}'")
                        if end_time is not None:
                            end_iso = end_time.isoformat()
                            conditions.append(f"timestamp <= '{end_iso}'")
                        if conditions:
                            time_filter = " AND ".join(conditions)
                    
                    sparse_frames = []
                    for q in sparse_queries:
                        sparse_results = text_retriever.retrieve_sparse(
                            query=q,
                            top_k=top_k,
                            text_field="text",
                            filter=time_filter
                        )
                        
                        for result in sparse_results:
                            score = result.get("_relevance_score") or result.get("_score") or result.get("score", 0.0)
                            if score == 0.0:
                                continue
                            
                            fid = result.get("frame_id")
                            if not fid:
                                continue
                            
                            frame = {
                                "frame_id": fid,
                                "timestamp": datetime.fromisoformat(result.get("timestamp")) if isinstance(result.get("timestamp"), str) else result.get("timestamp"),
                                "image_path": result.get("image_path"),
                                "ocr_text": result.get("text") or result.get("ocr_text", ""),
                                "distance": 1.0 - score,
                                "metadata": result.get("metadata", {})
                            }
                            
                            if frame.get("image_path"):
                                self._load_frame_image(frame)
                            
                            sparse_frames.append(frame)
                    
                    return sparse_frames
                except Exception as e:
                    logger.error(f"Sparse æ£€ç´¢å¤±è´¥: {e}", exc_info=True)
                    self.progress_signal.emit(f"âš ï¸ Sparse æ£€ç´¢å¤±è´¥ï¼Œä»…ä½¿ç”¨ Dense ç»“æœ")
                    return []
            
            # å¹¶è¡Œæ‰§è¡Œ Dense å’Œ Sparse æœç´¢ï¼ˆä½¿ç”¨ asyncioï¼‰
            if config.ENABLE_HYBRID:
                self.progress_signal.emit("ğŸ” å¯ç”¨ Hybrid Searchï¼Œå¹¶è¡Œæ‰§è¡Œ Dense å’Œ Sparse æ£€ç´¢...")
                async def _run_parallel_searches():
                    dense_task = asyncio.to_thread(_dense_search_task)
                    sparse_task = asyncio.to_thread(_sparse_search_task)
                    return await asyncio.gather(dense_task, sparse_task)
                
                dense_results, sparse_results = asyncio.run(_run_parallel_searches())
            else:
                dense_results = _dense_search_task()
                sparse_results = []
            
            # åˆå¹¶ç»“æœå¹¶å»é‡
            frames = []
            seen = set()
            
            for r in dense_results:
                fid = r.get("frame_id")
                if fid in seen:
                    continue
                seen.add(fid)
                frames.append(r)
            
            for r in sparse_results:
                fid = r.get("frame_id")
                if fid in seen:
                    continue
                seen.add(fid)
                frames.append(r)
            
            if not frames:
                self.result_signal.emit("åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°ç›¸å…³çš„å±å¹•è®°å½•ã€‚")
                return
            
            self.progress_signal.emit(f"ğŸ“Š æ£€ç´¢åˆ° {len(frames)} å¼ ç›¸å…³å›¾ç‰‡")
            
            # åŠ è½½å›¾ç‰‡
            for frame in frames:
                if 'image' not in frame or frame['image'] is None:
                    self._load_frame_image(frame)
            
            # ç¡®ä¿æ‰€æœ‰ frame éƒ½æœ‰å›¾ç‰‡
            frames_with_images = [f for f in frames if f.get("image") is not None]
            if not frames_with_images:
                self.result_signal.emit("æ£€ç´¢åˆ°çš„å›¾ç‰‡æ— æ³•åŠ è½½ã€‚")
                return
            
            # Rerank ç¯èŠ‚ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.ENABLE_RERANK:
                self.progress_signal.emit(f"ğŸ”„ æ­£åœ¨è¿›è¡Œ Rerankï¼ˆè¿”å› top-{config.RERANK_TOP_K}ï¼‰...")
                reranker = Reranker()
                frames_with_images = reranker.rerank(
                    query=query_text,
                    frames=frames_with_images,
                    top_k=config.RERANK_TOP_K
                )
                
                if not frames_with_images:
                    self.result_signal.emit("Rerank åæ²¡æœ‰å›¾ç‰‡ï¼Œæ— æ³•è¿›è¡Œ VLM åˆ†æã€‚")
                    return
                
                self.progress_signal.emit(f"âœ… Rerank å®Œæˆ: è¿”å› top-{len(frames_with_images)} å¼ å›¾ç‰‡")
            
            # VLM åˆ†æï¼ˆåªä½¿ç”¨é€šè¿‡ rerank çš„å›¾ç‰‡ï¼‰
            response = self._analyze_with_vlm(query_text, frames_with_images)
            result = self._format_rag_result(response, frames_with_images)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"RAGæ—¶é—´èŒƒå›´æŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"æŸ¥è¯¢å¤±è´¥: {str(e)}")
    
    def _rag_simple_mode(self, query_text: str, top_k: int = None) -> List[Dict]:
        """Simpleæ¨¡å¼çš„RAGæ£€ç´¢"""
        if top_k is None:
            top_k = config.MAX_IMAGES_TO_LOAD
        
        self.progress_signal.emit("ğŸ“‚ Simpleæ¨¡å¼: æ›´æ–°ç¼“å­˜...")
        new_frames_count = self.frame_cache.update(self.storage)
        if new_frames_count > 0:
            self.progress_signal.emit(f"âœ¨ å‘ç° {new_frames_count} å¼ æ–°å›¾ç‰‡")
        
        frames = self.frame_cache.get_frames()
        
        if not frames:
            return []
        
        self.progress_signal.emit(f"ğŸ“Š å½“å‰ç¼“å­˜: {len(frames)} å¼ å›¾ç‰‡")
        
        if len(frames) > top_k:
            frames = frames[:top_k]
        
        # å¸§å·®è¿‡æ»¤
        if config.ENABLE_QUERY_FRAME_DIFF:
            from query import _apply_frame_diff_filter
            self.progress_signal.emit("ğŸ” åº”ç”¨å¸§å·®è¿‡æ»¤...")
            filtered_frames = _apply_frame_diff_filter(frames)
            removed_count = len(frames) - len(filtered_frames)
            if removed_count > 0:
                self.progress_signal.emit(f"å·²è¿‡æ»¤ {removed_count} å¼ ç›¸ä¼¼å›¾ç‰‡")
            frames = filtered_frames
        
        return frames
    
    def _rag_vector_mode(self, query_text: str, top_k: int = None) -> List[Dict]:
        """Vectoræ¨¡å¼çš„RAGæ£€ç´¢"""
        if top_k is None:
            top_k = config.MAX_IMAGES_TO_LOAD
        
        self.progress_signal.emit(f"ğŸ” Vectoræ¨¡å¼: RAGè¯­ä¹‰æ£€ç´¢ top {top_k}...")
        
        frames = self.retriever.retrieve_by_text(query_text, top_k=top_k)
        
        if not frames:
            return []
        
        self.progress_signal.emit(f"âœ… æ£€ç´¢åˆ° {len(frames)} å¼ ç›¸å…³å›¾ç‰‡")
        
        # åŠ è½½å›¾ç‰‡
        for frame in frames:
            if 'image' not in frame or frame['image'] is None:
                self._load_frame_image(frame)
        
        return frames
    
    def _load_frame_image(self, frame: Dict):
        """åŠ è½½å¸§å›¾ç‰‡ï¼ˆæ”¯æŒå¤šç§è·¯å¾„æ ¼å¼ï¼‰"""
        try:
            image_path = None
            
            # 1. é¦–å…ˆå°è¯•ä½¿ç”¨ frame ä¸­çš„ image_path
            if 'image_path' in frame and frame['image_path']:
                path = Path(frame['image_path'])
                # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ä¸”å­˜åœ¨ï¼Œç›´æ¥ä½¿ç”¨
                if path.is_absolute() and path.exists():
                    image_path = path
                # å¦‚æœæ˜¯ç›¸å¯¹è·¯å¾„ï¼Œå°è¯•åœ¨å½“å‰ç›®å½•ä¸‹æŸ¥æ‰¾
                elif not path.is_absolute() and path.exists():
                    image_path = path
                # å°è¯•åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹æŸ¥æ‰¾ç›¸å¯¹è·¯å¾„
                elif not path.is_absolute():
                    abs_path = Path.cwd() / path
                    if abs_path.exists():
                        image_path = abs_path
            
            # 2. å¦‚æœä¸Šé¢éƒ½æ‰¾ä¸åˆ°ï¼Œå°è¯•æ ¹æ® frame_id å’Œ timestamp æ„å»ºè·¯å¾„
            if image_path is None and 'frame_id' in frame and 'timestamp' in frame:
                date_str = frame['timestamp'].strftime("%Y%m%d")
                # å°è¯•æ–°æ ¼å¼æ–‡ä»¶å
                new_path = Path(config.IMAGE_STORAGE_PATH) / date_str / f"{frame['frame_id']}.jpg"
                if new_path.exists():
                    image_path = new_path
            
            # 3. åŠ è½½å›¾ç‰‡
            if image_path and image_path.exists():
                frame['image'] = PILImage.open(image_path)
                logger.debug(f"æˆåŠŸåŠ è½½å›¾ç‰‡: {image_path}")
            else:
                original_path = frame.get('image_path', 'unknown')
                logger.warning(f"å›¾ç‰‡ä¸å­˜åœ¨: {original_path}")
                
        except Exception as e:
            logger.error(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
    
    def _analyze_with_vlm(self, query_text: str, frames: List[Dict]) -> str:
        """ä½¿ç”¨VLMåˆ†æå¸§"""
        self.progress_signal.emit(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨VLMåˆ†æ {len(frames)} å¼ å›¾ç‰‡...")
        
        # System prompt: å®šä¹‰åŠ©æ‰‹è§’è‰²
        system_prompt = "You are a helpful visual assistant. You analyze screenshots to answer user questions. Always respond in Chinese (ä¸­æ–‡å›ç­”)."
        
        # User prompt: ç›´æ¥å›ç­”é—®é¢˜ï¼Œç„¶åç”¨å›¾ç‰‡å†…å®¹ä½œä¸ºæ”¯æŒ
        prompt = f"""User Question: {query_text}

Please directly answer the user's question first, then provide supporting evidence from the {len(frames)} screenshots below. Focus on what the user was doing and how the visual content relates to their question."""
        
        all_images = [frame['image'] for frame in frames if frame.get('image') is not None]
        
        if not all_images:
            raise ValueError("æ— æ³•åŠ è½½å›¾ç‰‡")
        
        # æå–æ—¶é—´æˆ³
        timestamps = [frame.get('timestamp') for frame in frames if frame.get('image') is not None]
        
        return self.vlm._call_vlm(
            prompt, 
            all_images, 
            num_images=len(all_images),
            image_timestamps=timestamps if timestamps else None,
            system_prompt=system_prompt
        )
    
    def _format_rag_result(self, response: str, frames: List[Dict]) -> str:
        """æ ¼å¼åŒ–RAGç»“æœ"""
        result = f"ğŸ“ VLM å›ç­”:\n\n{response}\n\n"
        result += "="*60 + "\n"
        result += f"æ£€ç´¢ä¿¡æ¯:\n"
        result += f"â€¢ æ¨¡å¼: {self.storage_mode.upper()}\n"
        result += f"â€¢ æ£€ç´¢æ–¹æ³•: {'ç¼“å­˜åŠ è½½' if self.storage_mode == 'simple' else 'RAGè¯­ä¹‰æ£€ç´¢'}\n"
        result += f"â€¢ å›¾ç‰‡æ•°é‡: {len(frames)}\n"
        
        if frames:
            oldest = frames[-1]['timestamp']
            newest = frames[0]['timestamp']
            result += f"â€¢ æ—¶é—´èŒƒå›´: {oldest} åˆ° {newest}\n"
            
            if self.storage_mode == "vector" and frames and '_distance' in frames[0]:
                result += f"â€¢ æœ€é«˜ç›¸ä¼¼åº¦: {1.0 - frames[0].get('_distance', 0):.3f}\n"
        
        return result
    
    def query_time_summary(self, start_time: datetime, end_time: datetime):
        """æ¨¡å¼2: æ—¶é—´æ®µæ€»ç»“"""
        try:
            # Ensure start_time and end_time are UTC for database queries
            if start_time and start_time.tzinfo is None:
                start_time = start_time.astimezone(timezone.utc)
            if end_time and end_time.tzinfo is None:
                end_time = end_time.astimezone(timezone.utc)

            self.progress_signal.emit(f"ğŸ“… æ­£åœ¨åŠ è½½ {start_time} åˆ° {end_time} çš„æˆªå›¾...")
            logger.info(f"æ—¶é—´æ®µæ€»ç»“: {start_time} - {end_time}")
            
            # ä» SQLite è·å–æ—¶é—´èŒƒå›´å†…çš„å¸§
            frames_with_images = self._load_frames_in_timerange(start_time, end_time)
            
            if not frames_with_images:
                self.result_signal.emit("è¯¥æ—¶é—´æ®µå†…æ²¡æœ‰æˆªå›¾è®°å½•æˆ–æ— æ³•åŠ è½½å›¾ç‰‡ã€‚")
                return
            
            # é™åˆ¶æ•°é‡
            if len(frames_with_images) > 20:
                self.progress_signal.emit(f"å›¾ç‰‡æ•°é‡è¾ƒå¤š,é‡‡æ ·åˆ°20å¼ ...")
                step = len(frames_with_images) // 20
                frames_with_images = frames_with_images[::step][:20]
            
            # VLM æ€»ç»“
            response = self._summarize_with_vlm(start_time, end_time, frames_with_images)
            result = self._format_summary_result(response, start_time, end_time, frames_with_images)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"æ—¶é—´æ®µæ€»ç»“å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"æ€»ç»“å¤±è´¥: {str(e)}")
    
    def _load_frames_in_timerange(self, start_time: datetime, end_time: datetime) -> List[Dict]:
        """åŠ è½½æ—¶é—´èŒƒå›´å†…çš„å¸§"""
        sqlite_storage = SQLiteStorage(db_path=config.OCR_DB_PATH)
        frames_meta = sqlite_storage.get_recent_frames(limit=1000)
        
        # è¿‡æ»¤æ—¶é—´èŒƒå›´
        filtered_frames = [
            f for f in frames_meta
            if start_time <= f['timestamp'] <= end_time
        ]
        
        if not filtered_frames:
            return []
        
        self.progress_signal.emit(f"æ‰¾åˆ° {len(filtered_frames)} å¼ æˆªå›¾")
        
        # åŠ è½½å›¾ç‰‡
        frames_with_images = []
        for frame_meta in filtered_frames:
            try:
                image_path = Path(frame_meta['image_path'])
                if image_path.exists():
                    image = PILImage.open(image_path)
                    frames_with_images.append({
                        'image': image,
                        'timestamp': frame_meta['timestamp'],
                        'ocr_text': frame_meta.get('ocr_text', '')
                    })
            except Exception as e:
                logger.warning(f"åŠ è½½å›¾ç‰‡å¤±è´¥: {e}")
        
        return frames_with_images
    
    def _summarize_with_vlm(self, start_time: datetime, end_time: datetime, 
                           frames: List[Dict]) -> str:
        """ä½¿ç”¨VLMæ€»ç»“æ—¶é—´æ®µ"""
        self.progress_signal.emit(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨VLMæ€»ç»“ {len(frames)} å¼ æˆªå›¾...")
        
        # System prompt: å®šä¹‰åŠ©æ‰‹è§’è‰²
        system_prompt = "You are a helpful visual assistant. You analyze screenshots to provide summaries. Always respond in Chinese (ä¸­æ–‡å›ç­”)."
        
        # User prompt: ç›´æ¥æ€»ç»“ï¼Œç„¶åç”¨å›¾ç‰‡å†…å®¹ä½œä¸ºæ”¯æŒ
        prompt = f"""Please provide a comprehensive summary of what the user was doing from {start_time.strftime('%Y-%m-%d %H:%M:%S')} to {end_time.strftime('%Y-%m-%d %H:%M:%S')}.

Please directly provide the summary first, then use the {len(frames)} screenshots below as supporting evidence. Focus on:
1. Main activities and tasks
2. Applications and software used
3. Important work or browsing content
4. Overall time allocation"""
        
        all_images = [frame['image'] for frame in frames]
        
        # æå–æ—¶é—´æˆ³
        timestamps = [frame.get('timestamp') for frame in frames]
        
        return self.vlm._call_vlm(
            prompt, 
            all_images, 
            num_images=len(all_images),
            image_timestamps=timestamps if timestamps else None,
            system_prompt=system_prompt
        )
    
    def _format_summary_result(self, response: str, start_time: datetime, 
                               end_time: datetime, frames: List[Dict]) -> str:
        """æ ¼å¼åŒ–æ€»ç»“ç»“æœ"""
        result = f"ğŸ“ æ—¶é—´æ®µæ€»ç»“:\n\n{response}\n\n"
        result += "="*60 + "\n"
        result += f"æ—¶é—´èŒƒå›´: {start_time.strftime('%Y-%m-%d %H:%M:%S')} åˆ° {end_time.strftime('%Y-%m-%d %H:%M:%S')}\n"
        result += f"åˆ†æäº† {len(frames)} å¼ æˆªå›¾\n"
        return result
    
    def query_realtime(self, question: str):
        """æ¨¡å¼3: å®æ—¶é—®ç­”"""
        try:
            self.progress_signal.emit("ğŸ“¸ æ­£åœ¨è·å–å½“å‰å±å¹•å’Œå†å²æˆªå›¾...")
            logger.info(f"å®æ—¶é—®ç­”: '{question}'")
            
            # å®æ—¶é—®ç­”ä¸éœ€è¦åˆå§‹åŒ– storage æˆ– CLIP æ¨¡å‹
            # ç›´æ¥ä½¿ç”¨ SQLite è·å–å†å²æˆªå›¾
            
            # è·å–å½“å‰å’Œå†å²å›¾ç‰‡
            images, timestamps = self._get_realtime_images()
            
            if not images:
                self.error_signal.emit("æ— æ³•è·å–å±å¹•æˆªå›¾")
                return
            
            # VLMåˆ†æ
            response = self._analyze_realtime_with_vlm(question, images, timestamps)
            result = self._format_realtime_result(response, images)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"å®æ—¶é—®ç­”å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"é—®ç­”å¤±è´¥: {str(e)}")
    
    def _get_realtime_images(self) -> tuple[List[PILImage.Image], List[datetime]]:
        """è·å–å½“å‰å’Œå†å²å›¾ç‰‡åŠå…¶æ—¶é—´æˆ³"""
        # 1. å½“å‰å±å¹•æˆªå›¾
        capturer = ScreenshotCapturer()
        current_frame = capturer.capture()
        
        if not current_frame:
            return [], []
        
        images = [current_frame.image]
        timestamps = [current_frame.timestamp]
        
        # 2. ä» SQLite è·å–æœ€è¿‘5å¼ æˆªå›¾ï¼ˆä¸ä¾èµ– storage çš„ load_recent æ–¹æ³•ï¼‰
        self.progress_signal.emit("ğŸ“‚ åŠ è½½æœ€è¿‘5å¼ æˆªå›¾...")
        
        try:
            sqlite_storage = SQLiteStorage(db_path=config.OCR_DB_PATH)
            recent_frames = sqlite_storage.get_recent_frames(limit=5)
            
            for frame_meta in recent_frames:
                try:
                    image_path = Path(frame_meta['image_path'])
                    if image_path.exists():
                        image = PILImage.open(image_path)
                        images.append(image)
                        timestamps.append(frame_meta['timestamp'])
                except Exception as e:
                    logger.warning(f"åŠ è½½å†å²æˆªå›¾å¤±è´¥: {e}")
        except Exception as e:
            logger.warning(f"ä»SQLiteè·å–å†å²æˆªå›¾å¤±è´¥: {e}")
        
        return images, timestamps
    
    def _analyze_realtime_with_vlm(self, question: str, images: List[PILImage.Image], timestamps: List[datetime]) -> str:
        """ä½¿ç”¨VLMåˆ†æå®æ—¶é—®é¢˜"""
        self.progress_signal.emit(f"ğŸ¤– æ­£åœ¨ä½¿ç”¨VLMåˆ†æ {len(images)} å¼ å›¾ç‰‡...")
        
        # System prompt: å®šä¹‰åŠ©æ‰‹è§’è‰²
        system_prompt = "You are a helpful visual assistant. You analyze screenshots to answer user questions. Always respond in Chinese (ä¸­æ–‡å›ç­”)."
        
        # User prompt: ç›´æ¥å›ç­”é—®é¢˜
        prompt = f"""User Question: {question}

The first image is the current screen, and the remaining images are recent history. Please directly answer the user's question first, then provide supporting evidence from the screenshots."""
        
        return self.vlm._call_vlm(
            prompt, 
            images, 
            num_images=len(images),
            image_timestamps=timestamps,
            system_prompt=system_prompt
        )
    
    def _format_realtime_result(self, response: str, images: List[PILImage.Image]) -> str:
        """æ ¼å¼åŒ–å®æ—¶é—®ç­”ç»“æœ"""
        result = f"ğŸ“ å®æ—¶é—®ç­”ç»“æœ:\n\n{response}\n\n"
        result += "="*60 + "\n"
        result += f"åˆ†æäº† {len(images)} å¼ å›¾ç‰‡ (1å¼ å½“å‰ + {len(images)-1}å¼ å†å²)\n"
        return result
    
    # ============ OCR æ¨¡å¼æŸ¥è¯¢ ============
    
    def query_ocr_rag(self, query_text: str, start_time: datetime, end_time: datetime):
        """OCR æ¨¡å¼ RAG æŸ¥è¯¢ - åŸºäº OCR æ–‡æœ¬ embedding æ£€ç´¢ï¼ˆä½¿ç”¨ LanceDB Pre-filteringï¼‰"""
        try:
            # Ensure start_time and end_time are UTC for database queries
            if start_time and start_time.tzinfo is None:
                start_time = start_time.astimezone(timezone.utc)
            if end_time and end_time.tzinfo is None:
                end_time = end_time.astimezone(timezone.utc)

            self.progress_signal.emit("ğŸ“ OCRæ¨¡å¼: æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æœ¬...")
            logger.info(f"OCR RAGæŸ¥è¯¢: '{query_text}' ({start_time} - {end_time})")
            
            # ä½¿ç”¨ CLIP å¯¹æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œ embedding
            self._ensure_vector_mode_initialized()
            
            # è·å–æŸ¥è¯¢æ–‡æœ¬çš„ embedding
            query_embedding = self.encoder.encode_text(query_text)
            
            # åœ¨ OCR è¡¨ä¸­æœç´¢ï¼ˆä½¿ç”¨ LanceDB Pre-filteringï¼Œç›´æ¥ä¼ é€’æ—¶é—´èŒƒå›´ï¼‰
            frames = self.storage.search_ocr(
                query_embedding, 
                top_k=20,
                start_time=start_time,
                end_time=end_time
            )
            
            if not frames:
                # å¦‚æœ OCR è¡¨ä¸ºç©ºï¼Œå›é€€åˆ° SQLite å…¨æ–‡æœç´¢
                self.progress_signal.emit("ğŸ“‚ OCRå‘é‡è¡¨ä¸ºç©ºï¼Œä½¿ç”¨å…¨æ–‡æœç´¢...")
                frames = self._search_ocr_fulltext(query_text, start_time, end_time)
            
            if not frames:
                self.result_signal.emit("åœ¨æŒ‡å®šæ—¶é—´èŒƒå›´å†…æœªæ‰¾åˆ°ç›¸å…³çš„OCRæ–‡æœ¬è®°å½•ã€‚")
                return
            
            self.progress_signal.emit(f"ğŸ“Š æ‰¾åˆ° {len(frames)} æ¡ç›¸å…³OCRè®°å½•")
            
            # ä½¿ç”¨çº¯æ–‡æœ¬ VLM åˆ†æ
            response = self._analyze_ocr_with_vlm(query_text, frames)
            result = self._format_ocr_result(response, frames)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"OCR RAGæŸ¥è¯¢å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"OCRæŸ¥è¯¢å¤±è´¥: {str(e)}")
    
    def _search_ocr_fulltext(self, query_text: str, start_time: datetime, end_time: datetime) -> List[Dict]:
        """ä½¿ç”¨ SQLite å…¨æ–‡æœç´¢ OCR æ–‡æœ¬"""
        try:
            sqlite_storage = SQLiteStorage(db_path=config.OCR_DB_PATH)
            
            # ä½¿ç”¨ SQLite çš„æ–‡æœ¬æœç´¢
            results = sqlite_storage.search_text(query_text, limit=20)
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´å¹¶æ„å»ºç»“æœ
            frames = []
            for r in results:
                ts = r.get('timestamp')
                if ts and start_time <= ts <= end_time:
                    frames.append({
                        'frame_id': r['frame_id'],
                        'timestamp': ts,
                        'image_path': r.get('image_path', ''),
                        'ocr_text': r.get('ocr_text', ''),
                        'image': None  # OCR æ¨¡å¼ä¸éœ€è¦åŠ è½½å›¾ç‰‡
                    })
            
            return frames
            
        except Exception as e:
            logger.error(f"å…¨æ–‡æœç´¢å¤±è´¥: {e}")
            return []
    
    def _analyze_ocr_with_vlm(self, query_text: str, frames: List[Dict]) -> str:
        """ä½¿ç”¨ VLM åˆ†æ OCR æ–‡æœ¬ï¼ˆçº¯æ–‡æœ¬æ¨¡å¼ï¼‰"""
        self.progress_signal.emit(f"ğŸ¤– æ­£åœ¨åˆ†æ {len(frames)} æ¡OCRæ–‡æœ¬...")
        
        # æ„å»º OCR æ–‡æœ¬ä¸Šä¸‹æ–‡
        ocr_context = []
        for i, frame in enumerate(frames[:15], 1):  # æœ€å¤š15æ¡
            ts = frame['timestamp'].strftime('%Y-%m-%d %H:%M:%S')
            text = frame.get('ocr_text', '')[:500]  # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
            ocr_context.append(f"[{i}] æ—¶é—´: {ts}\nå†…å®¹: {text}")
        
        ocr_text_block = "\n\n".join(ocr_context)
        
        # System prompt: å®šä¹‰åŠ©æ‰‹è§’è‰²
        system_prompt = "You are a helpful assistant. You analyze OCR text to answer user questions. Always respond in Chinese (ä¸­æ–‡å›ç­”)."
        
        # User prompt: ç›´æ¥å›ç­”é—®é¢˜
        prompt = f"""User Question: {query_text}

Please directly answer the user's question first, then provide supporting evidence from the OCR text below.

OCR Text Records (sorted by relevance):

{ocr_text_block}

If relevant content is found, please explain in detail:
1. At which time point the relevant content was seen
2. What the specific text content is
3. Possible context (what the user was doing at that time)"""
        
        return self.vlm._call_vlm_text_only(prompt, system_prompt=system_prompt)
    
    def _format_ocr_result(self, response: str, frames: List[Dict]) -> str:
        """æ ¼å¼åŒ– OCR æŸ¥è¯¢ç»“æœ"""
        result = f"ğŸ“ OCRæ–‡æœ¬æ£€ç´¢ç»“æœ:\n\n{response}\n\n"
        result += "="*60 + "\n"
        result += f"æ£€ç´¢ä¿¡æ¯:\n"
        result += f"â€¢ æ¨¡å¼: OCRæ–‡æœ¬æ£€ç´¢\n"
        result += f"â€¢ åŒ¹é…è®°å½•: {len(frames)} æ¡\n"
        
        if frames:
            result += f"\nç›¸å…³æ—¶é—´ç‚¹:\n"
            for i, frame in enumerate(frames[:5], 1):
                ts = frame['timestamp'].strftime('%H:%M:%S')
                text_preview = (frame.get('ocr_text', '')[:50] + "...") if len(frame.get('ocr_text', '')) > 50 else frame.get('ocr_text', '')
                result += f"  {i}. [{ts}] {text_preview}\n"
        
        return result
    
    def query_realtime_ocr(self, question: str):
        """OCR æ¨¡å¼å®æ—¶é—®ç­” - åŸºäºå½“å‰å’Œå†å² OCR æ–‡æœ¬"""
        try:
            self.progress_signal.emit("ğŸ“ OCRæ¨¡å¼: è·å–å½“å‰å’Œå†å²OCRæ–‡æœ¬...")
            logger.info(f"OCRå®æ—¶é—®ç­”: '{question}'")
            
            # è·å–å½“å‰å±å¹•çš„ OCR
            from core.capture.capturer import ScreenshotCapturer
            from core.ocr.paddleocr_worker import PaddleOCRWorker
            
            capturer = ScreenshotCapturer()
            current_frame = capturer.capture()
            
            ocr_texts = []
            
            if current_frame:
                # å¯¹å½“å‰å±å¹•è¿›è¡Œ OCR
                self.progress_signal.emit("ğŸ“¸ æ­£åœ¨è¯†åˆ«å½“å‰å±å¹•æ–‡å­—...")
                try:
                    ocr_worker = PaddleOCRWorker()
                    current_ocr = ocr_worker.process(current_frame.image)
                    if current_ocr:
                        ocr_texts.append({
                            'time': 'å½“å‰',
                            'text': current_ocr
                        })
                except Exception as e:
                    logger.warning(f"å½“å‰å±å¹•OCRå¤±è´¥: {e}")
            
            # è·å–æœ€è¿‘çš„å†å² OCR æ–‡æœ¬
            self.progress_signal.emit("ğŸ“‚ åŠ è½½æœ€è¿‘çš„OCRè®°å½•...")
            try:
                sqlite_storage = SQLiteStorage(db_path=config.OCR_DB_PATH)
                recent_frames = sqlite_storage.get_recent_frames(limit=10)
                
                for frame in recent_frames:
                    ocr_text = frame.get('ocr_text', '')
                    if ocr_text:
                        # å°†å­˜å‚¨çš„ UTC æ—¶é—´è½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´
                        ts_obj = frame['timestamp']
                        if ts_obj.tzinfo is None:
                            ts_obj = ts_obj.replace(tzinfo=timezone.utc)
                        ts = ts_obj.astimezone().strftime('%H:%M:%S')
                        
                        ocr_texts.append({
                            'time': ts,
                            'text': ocr_text[:500]  # æˆªæ–­
                        })
            except Exception as e:
                logger.warning(f"è·å–å†å²OCRå¤±è´¥: {e}")
            
            if not ocr_texts:
                self.error_signal.emit("æ— æ³•è·å–OCRæ–‡æœ¬")
                return
            
            # ä½¿ç”¨ VLM åˆ†æ
            response = self._analyze_realtime_ocr_with_vlm(question, ocr_texts)
            result = self._format_realtime_ocr_result(response, ocr_texts)
            
            self.result_signal.emit(result)
            
        except Exception as e:
            logger.error(f"OCRå®æ—¶é—®ç­”å¤±è´¥: {e}", exc_info=True)
            self.error_signal.emit(f"OCRé—®ç­”å¤±è´¥: {str(e)}")
    
    def _analyze_realtime_ocr_with_vlm(self, question: str, ocr_texts: List[Dict]) -> str:
        """ä½¿ç”¨ VLM åˆ†æå®æ—¶ OCR æ–‡æœ¬"""
        self.progress_signal.emit(f"ğŸ¤– æ­£åœ¨åˆ†æ {len(ocr_texts)} æ¡OCRæ–‡æœ¬...")
        
        context_parts = []
        for item in ocr_texts:
            context_parts.append(f"[{item['time']}]\n{item['text']}")
        
        ocr_context = "\n\n---\n\n".join(context_parts)
        
        # System prompt: å®šä¹‰åŠ©æ‰‹è§’è‰²
        system_prompt = "You are a helpful assistant. You analyze OCR text to answer user questions. Always respond in Chinese (ä¸­æ–‡å›ç­”)."
        
        # User prompt: ç›´æ¥å›ç­”é—®é¢˜
        prompt = f"""User Question: {question}

Please directly answer the user's question first, then provide supporting evidence from the OCR text below.

OCR Text Extracted from Screenshots (first is current screen, others are historical records):

{ocr_context}"""
        
        return self.vlm._call_vlm_text_only(prompt, system_prompt=system_prompt)
    
    def _format_realtime_ocr_result(self, response: str, ocr_texts: List[Dict]) -> str:
        """æ ¼å¼åŒ–å®æ—¶ OCR é—®ç­”ç»“æœ"""
        result = f"ğŸ“ OCRå®æ—¶é—®ç­”ç»“æœ:\n\n{response}\n\n"
        result += "="*60 + "\n"
        result += f"åˆ†æäº† {len(ocr_texts)} æ¡OCRæ–‡æœ¬è®°å½•\n"
        return result



